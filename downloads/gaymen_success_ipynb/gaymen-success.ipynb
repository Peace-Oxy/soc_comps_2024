{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b85a4727-27c4-40a3-8925-cb0ebffea6da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T17:25:04.141427Z",
     "iopub.status.busy": "2024-11-12T17:25:04.141160Z",
     "iopub.status.idle": "2024-11-12T17:25:10.128625Z",
     "shell.execute_reply": "2024-11-12T17:25:10.128129Z",
     "shell.execute_reply.started": "2024-11-12T17:25:04.141406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Using cached praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Using cached prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Using cached update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.11/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.7.4)\n",
      "Using cached praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Using cached prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Collecting https://github.com/praw-dev/praw/archive/master.zip\n",
      "  Using cached https://github.com/praw-dev/praw/archive/master.zip\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: prawcore<3,>=2.4 in /opt/conda/lib/python3.11/site-packages (from praw==7.8.2.dev0) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /opt/conda/lib/python3.11/site-packages (from praw==7.8.2.dev0) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/conda/lib/python3.11/site-packages (from praw==7.8.2.dev0) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from prawcore<3,>=2.4->praw==7.8.2.dev0) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw==7.8.2.dev0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw==7.8.2.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw==7.8.2.dev0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw==7.8.2.dev0) (2024.7.4)\n",
      "Building wheels for collected packages: praw\n",
      "  Building wheel for praw (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for praw: filename=praw-7.8.2.dev0-py3-none-any.whl size=189388 sha256=afe901dd664f0ff94028d9ea2f6afad4fa8917afe32240cffee47ad3913baf0e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-y9j6dtx4/wheels/f5/5b/3e/81dcf511298e5fcd6a4147a8f9c19568acb59be4a72eef36ba\n",
      "Successfully built praw\n",
      "Installing collected packages: praw\n",
      "  Attempting uninstall: praw\n",
      "    Found existing installation: praw 7.8.1\n",
      "    Uninstalling praw-7.8.1:\n",
      "      Successfully uninstalled praw-7.8.1\n",
      "Successfully installed praw-7.8.2.dev0\n"
     ]
    }
   ],
   "source": [
    "## download praw \n",
    "!pip install praw\n",
    "!pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip\n",
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41aa82b6-e761-441b-9f8c-a409ce0cf473",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T17:25:15.370515Z",
     "iopub.status.busy": "2024-11-12T17:25:15.369924Z",
     "iopub.status.idle": "2024-11-12T17:25:26.192959Z",
     "shell.execute_reply": "2024-11-12T17:25:26.192511Z",
     "shell.execute_reply.started": "2024-11-12T17:25:15.370491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to reddit_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Set up Reddit API client using praw\n",
    "reddit = praw.Reddit(\n",
    "    client_id='dc3EBg9kKK2_OyV5xWqJXQ',  # Replace with your client_id\n",
    "    client_secret='HlKaJW1s-jdGR5u5eXbihDg6FE9Lbg',  # Replace with your client_secret\n",
    "    user_agent='seniorcomps',  # Replace with your user_agent\n",
    "    username='rbeal@oxy.edu',  # Replace with your Reddit username (optional)\n",
    ")\n",
    "\n",
    "def scrape_subreddit_to_csv(subreddit_name, exclude_words, post_limit=50, replies_limit=5, output_file=\"reddit_data.csv\"):\n",
    "    \"\"\"\n",
    "    Scrapes subreddit posts and top replies, excluding posts with specific words, pictures, or links,\n",
    "    and saves the results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        subreddit_name (str): Name of the subreddit to scrape.\n",
    "        exclude_words (list): List of words to exclude posts by.\n",
    "        post_limit (int): Number of posts to retrieve.\n",
    "        replies_limit (int): Number of top replies per post to retrieve.\n",
    "        output_file (str): Name of the CSV file to save the data.\n",
    "    \"\"\"\n",
    "    # List to store the data\n",
    "    data = []\n",
    "\n",
    "    # Access the subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "\n",
    "    # Iterate over the subreddit posts\n",
    "    for post in subreddit.new(limit=post_limit):\n",
    "        # Filter out posts containing excluded words, images, or links\n",
    "        if (any(word.lower() in post.title.lower() or word.lower() in post.selftext.lower() for word in exclude_words) \n",
    "                or not post.is_self  # Exclude non-text posts (images, links)\n",
    "                or 'http' in post.selftext.lower()):  # Exclude posts containing links\n",
    "            continue  # Skip posts that meet exclusion criteria\n",
    "\n",
    "        # Iterate through post's top-level comments and get top replies\n",
    "        post.comments.replace_more(limit=0)  # Load all top-level comments\n",
    "        top_replies = post.comments[:replies_limit]\n",
    "        \n",
    "        # Collect post and replies data\n",
    "        for reply in top_replies:\n",
    "            data.append({\n",
    "                \"post_title\": post.title,\n",
    "                \"post_body\": post.selftext,\n",
    "                \"post_url\": post.url,\n",
    "                \"post_score\": post.score,\n",
    "                \"post_id\": post.id,\n",
    "                \"post_author\": str(post.author),\n",
    "                \"post_created\": post.created_utc,\n",
    "                \"post_num_comments\": post.num_comments,\n",
    "                \"reply_author\": str(reply.author),\n",
    "                \"reply_body\": reply.body,\n",
    "                \"reply_score\": reply.score,\n",
    "                \"reply_created\": reply.created_utc\n",
    "            })\n",
    "    \n",
    "    # Save data to a CSV file using pandas\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "excluded_words = [\"biden\", \"trump\", \"harris\", \"election\"]  # Words to exclude\n",
    "subreddit_name = \"librarians\"  # Subreddit to scrape\n",
    "output_file = \"reddit_data.csv\"\n",
    "scrape_subreddit_to_csv(subreddit_name, excluded_words, post_limit=50, replies_limit=5, output_file=output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac946410-5429-485e-a60f-563cb3dff31a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
